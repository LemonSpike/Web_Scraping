{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txtClean(txt):\n",
    "    txt = str(txt)\n",
    "    txt = cleanhtml(txt)\n",
    "    txt = txt.lower()\n",
    "    #txt = re.sub(r'http\\S+', '', txt) #remove url\n",
    "    #txt = re.sub(r'pic\\S+', '', txt)\n",
    "    #txt = re.sub(r'@', '', txt)\n",
    "    #txt = txt.replace('’',' ')\n",
    "    txt = txt.replace('[',' ')\n",
    "    txt = txt.replace(']',' ')\n",
    "    #txt = txt.replace(r'\\xa0',' ',txt)\n",
    "    \n",
    "    #txt = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', txt, flags=re.MULTILINE)\n",
    "    \n",
    "    #txt = txt.replace('&amp','')\n",
    "    #txt = txt.replace('ly…','')\n",
    "    #txt = txt.replace('ht…','')\n",
    "    \n",
    "    '''\n",
    "    p = string.punctuation\n",
    "    d = string.digits\n",
    "    table_p = str.maketrans(p, len(p)*' ')\n",
    "    table_d = str.maketrans(d, len(d)*' ')\n",
    "    txt = txt.translate(table_p)\n",
    "    txt = txt.translate(table_d)\n",
    "    '''\n",
    "    \n",
    "    #words = nltk.word_tokenize(txt)\n",
    "    #words = [w for w in words if w not in stopwords]\n",
    "    #tag = nltk.pos_tag(words)\n",
    "    #for k,v in tag:\n",
    "        #words = [lmtzr.lemmatize(k, getPos(v)) for k in words]    \n",
    "    #cleaned = [w for w in words if w not in stopwords]\n",
    "    #final = ' '.join(cleaned)\n",
    "    final = txt\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeClean(t):\n",
    "    #t = re.search(r'(?<=title=\\\")(.*)(?=\\\")', t).group(0)\n",
    "    t = re.search(r'title\\=\\\"(.+?)\\\"', str(t)).group(0)\n",
    "    t = re.sub(r'title=','',t)\n",
    "    t = re.sub(r'\\\"', '', t)\n",
    "    t = t.split(' - ')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the id of this tweet from the full raw tweet\n",
    "def tweetidFind(fulltweet):\n",
    "    t = re.search(r'data-item-id=\\\"(.+?)\\\"', str(fulltweet)).group(0)\n",
    "    t = re.sub(r'data-item-id=\\\"','',t)\n",
    "    t = re.sub(r'\\\"', '', t)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def screenClean(name):\n",
    "    t = re.search(r'\\<b\\>(.+?)\\<\\/b\\>', str(name)).group(0)\n",
    "    #t=re.sub(r'part=\"\"','',t)\n",
    "    t = re.sub(r'\\<b\\>', '', t)\n",
    "    t = re.sub(r'\\<\\/b\\>', '', t)\n",
    "    t = '@'+t\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the real screenname of this tweet\n",
    "def trueName(name):\n",
    "    for n in name:\n",
    "        if 'data-aria-label-part' in str(n):\n",
    "            return screenClean(n)\n",
    "        else:\n",
    "            return 'NameError'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def userClean(user):\n",
    "    t=re.search(r'part\\=\\\"\\\"\\>(.+?)\\<', str(user)).group(0)\n",
    "    t=re.sub(r'part=\"\"','',t)\n",
    "    t = re.sub(r'\\>', '', t)\n",
    "    t = re.sub(r'\\<', '', t)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/intelkasetti/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: use options instead of chrome_options\n",
      "  if __name__ == '__main__':\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting tweets from 2015-01-02 to 2015-01-03\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "f = open('tweets_selenium.txt','w', encoding='utf-8')\n",
    "c = csv.writer(f)\n",
    "c.writerow(['id', 'time', 'date', 'username', 'screenname', 'text'])\n",
    "\n",
    "try: #use try function to avoid error. This is not necessary\n",
    "    #set browser and browser driver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('window-size=1200x600')\n",
    "    driver = webdriver.Chrome(executable_path='/Users/intelkasetti/Desktop/chromedriver', chrome_options=options) #path of driver\n",
    "    \n",
    "    date = datetime.datetime(2015,1,1,0,0,0) #starting date\n",
    "    while date <= datetime.datetime(2019,12,31,0,0,0): #stop date\n",
    "        keyword1 = 'Trump'\n",
    "        keyword2 = 'President' #add more keywords as you wish\n",
    "        url = 'https://twitter.com/search?f=tweets&q='+keyword1+'%20'+keyword2+'%20since%3A'+\\\n",
    "        str(date)[:10]+'%20until%3A'+str(date+datetime.timedelta(days=1))[:10]+'&src=typd&lang=en'\n",
    "        #add '%20' between every keyword\n",
    "        date += datetime.timedelta(days=1) #set the days of increment\n",
    "    \n",
    "        #load the webpage\n",
    "        driver.get(url)\n",
    "        \n",
    "        #start scroll function\n",
    "        SCROLL_PAUSE_TIME = 4\n",
    "\n",
    "        # Get scroll height\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "        print('collecting tweets from '+str(date)[:10]+' to '+str(date+datetime.timedelta(days=1))[:10])\n",
    "    \n",
    "        with tqdm() as pbar:\n",
    "            while True:\n",
    "                # Scroll down to bottom\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            \n",
    "                # Wait to load page\n",
    "                time.sleep(SCROLL_PAUSE_TIME)\n",
    "            \n",
    "                # Calculate new scroll height and compare with last scroll height\n",
    "                new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                if new_height == last_height:\n",
    "                    break\n",
    "                last_height = new_height\n",
    "                pbar.update()\n",
    "        #end scroll function\n",
    "        \n",
    "        html_source = driver.page_source #save html file of target page\n",
    "        sourcedata = html_source.encode('utf-8') #save the page content into readable text\n",
    "        soup = BeautifulSoup(sourcedata, 'html.parser') #make the page a beautifulcoup object\n",
    "        \n",
    "        #locate all tweets\n",
    "        raw_tweet = soup.body.find_all('li', {'class':\"js-stream-item stream-item stream-item \"})\n",
    "        \n",
    "        for i in range(len(raw_tweet)):\n",
    "            td = raw_tweet[i].find_all('a', {'class':'tweet-timestamp'})\n",
    "            texts = raw_tweet[i].find_all('p',{'TweetTextSize'})\n",
    "            usr_name = raw_tweet[i].find_all('span', {'class':'FullNameGroup'})\n",
    "            screen_name = raw_tweet[i].find_all('span',{'class':'username u-dir u-textTruncate'})\n",
    "            \n",
    "            tweet_id = tweetidFind(raw_tweet[i])\n",
    "            \n",
    "            time_list = timeClean(str(td))\n",
    "            usr = userClean(usr_name)\n",
    "            sname = screenClean(screen_name)\n",
    "            text = txtClean(texts)\n",
    "            row = [tweet_id, time_list[0], time_list[1], usr,\n",
    "                   sname, text]\n",
    "            c.writerow(row)\n",
    "except Exception as e:\n",
    "    pass\n",
    "        \n",
    "f.close()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
